{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Unity environment for ChickAI: virtual controlled-rearing experiments This is a collection of tools for simulating virtual agents under controlled-rearing conditions. The agents generated and studied through this pipeline can be compared directly to real chicks recorded by the Building a Mind Lab . This pipeline provides all necessary components for simulating and replicating embodied models from the lab. The figure below shows the experiment setup for the three experiments discussed in the guide. How to Use this Repository This directory provides three components for building embodied virtual agents. These are a video game which serves as a virtual world, a set of programs to run experiments in the virtual world, and a set of programs to visualize the data coming from the experiments. Once users download this repo they will most likely need to open Unity at least once to generate an executable of the environment. After an executable is available, the user should be able to run the necessary simulations. This will result in data that can be analyzed using the scripts in the analysis folder. If users are unfamiliar with how to install a git repository or have never used unity before please scroll down to how to the how to install section. Directory Structure Following the directory structure of the code. \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 executables \u251c\u2500\u2500 docs \u251c\u2500\u2500 mkdocs.yml \u251c\u2500\u2500 README.md \u251c\u2500\u2500 requirements.txt \u251c\u2500\u2500 scripts \u2502 \u251c\u2500\u2500 parsing_sup.sh \u2502 \u251c\u2500\u2500 parsing_unsup.sh \u2502 \u2514\u2500\u2500 viewpoint_sup.sh \u251c\u2500\u2500 src \u2502 \u251c\u2500\u2500 analysis \u2502 \u2514\u2500\u2500 simulation \u2514\u2500\u2500 tests src/analysis : Contains the code for visualizing and analyzing results from the simulation experiments. data : This folder contains executables, experimental results, and analysis results. A user will rarely interact with this folder directly. Most scripts assume this directory and all its contents exist. src/simulation : Contains the code for running experiments with simulated agents. Following is the structure of simulation folder: ``` \u251c\u2500\u2500 agent \u251c\u2500\u2500 algorithms \u251c\u2500\u2500 callback \u251c\u2500\u2500 common \u251c\u2500\u2500 conf \u251c\u2500\u2500 env_wrapper \u251c\u2500\u2500 networks \u251c\u2500\u2500 pycache \u251c\u2500\u2500 run_parsing_exp.py \u251c\u2500\u2500 run_parsing_icm.py \u251c\u2500\u2500 run_viewpoint_exp.py \u2514\u2500\u2500 utils.py `` * Unity : Contains a Unity project which is a virtual replication of the VR Chambers used in contolled-rearing studies from the lab. This folder should be opened as a Unity project in the Unity Editor. * tests : Contains unit tests to test the environment and others * docs : Contains project documentation * scripts`: Contains bash scripts to run the respective experiment How to Install In this section, you will pull this repository from Github, open the Unity environment, and build the ChickAI environment as an executable. Codebase Installation Install Git and/or Github Desktop. If you install Git, you'll be able to interact with Github through the command line. You can download Git using the directions here: https://git-scm.com/downloads . If you install Git Desktop, you can use a GUI to interact with Github. You can install Github Desktop by following the directions here: https://docs.github.com/en/desktop/installing-and-configuring-github-desktop/ . For the following steps, I will provide the command line arguments (but you can use the GUI to find the same options in Github Desktop). To download the repository, click the Code button on the pipeline_embodied repo. Copy the provided web URL. Then follow the code below to change to directory where you want the repo (denoted here as MY_FOLDER) and then clone the repo. cd MY_FOLDER git clone URL_YOU_COPIED_GOES_HERE 3. Checkout the branch you want to be extra sure that you're using the right branch. cd pipeline_embodied git checkout DESIRED_BRANCH 4. (Highly Recommended) create and configure a virtual environment *steps described below : * ``` conda create -n pipeline_embodied_env python=3.8 conda activate pipeline_embodied_env git clone git@github.com:buildingamind/pipeline_embodied.git cd pipeline_embodied pip install -r requirements.txt ``` Running an Experiment (default configuration) After having followed steps 1-5 above once experiments can be run with a few lines of code bash scripts/<EXPERIMENT_NAME>_sup.sh where EXPERIMENT_NAME is one of the experiments ( viewpoint , binding , parsing ) using supervised reward.** Running Standard Analysis After running the experiments, the pipeline will generate a collection of datafiles in the Data folder. To run the analyses performed in the papers you can use the following command python3 src/analysis/run.py This will generate a collection of graphs in the folder data/Results . Running an Experiment (custom configuration) After having replicated the results with the default configuration, you may wish to experiment by plugging in different brains for the agent. We have included a few different possible brains for the agent. To plug these in simply modify the yaml file in src/simulation/conf/Agent/basic.yaml . encoder: BRAIN where BRAIN can be set to small , medium , or large which correspond to a 4-layer CNN, 10-Layer ResNet, and 18-layer ResNet respectively. Note that if you change the size of the encoder, you may also consider changing the number of training episodes. This can be done in the config file src/simulation/conf/config.yaml . train_eps: NEW_EPISODE_COUNT If you wish to experiment with custom architectures or a new policy network, this can be done by modifying the agent script ( Simulation/agent.py ). self.model is the policy network and self.encoder is the encoder network. Both can be assigned any appropriately sized torch.nn.module . Experiment Configuration More information related to details on the experiment can be found on following pages. Parsing Experiment ViewPoint Experiment","title":"Home"},{"location":"#unity-environment-for-chickai-virtual-controlled-rearing-experiments","text":"This is a collection of tools for simulating virtual agents under controlled-rearing conditions. The agents generated and studied through this pipeline can be compared directly to real chicks recorded by the Building a Mind Lab . This pipeline provides all necessary components for simulating and replicating embodied models from the lab. The figure below shows the experiment setup for the three experiments discussed in the guide.","title":"Unity environment for ChickAI: virtual controlled-rearing experiments"},{"location":"#how-to-use-this-repository","text":"This directory provides three components for building embodied virtual agents. These are a video game which serves as a virtual world, a set of programs to run experiments in the virtual world, and a set of programs to visualize the data coming from the experiments. Once users download this repo they will most likely need to open Unity at least once to generate an executable of the environment. After an executable is available, the user should be able to run the necessary simulations. This will result in data that can be analyzed using the scripts in the analysis folder. If users are unfamiliar with how to install a git repository or have never used unity before please scroll down to how to the how to install section.","title":"How to Use this Repository"},{"location":"#directory-structure","text":"Following the directory structure of the code. \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 executables \u251c\u2500\u2500 docs \u251c\u2500\u2500 mkdocs.yml \u251c\u2500\u2500 README.md \u251c\u2500\u2500 requirements.txt \u251c\u2500\u2500 scripts \u2502 \u251c\u2500\u2500 parsing_sup.sh \u2502 \u251c\u2500\u2500 parsing_unsup.sh \u2502 \u2514\u2500\u2500 viewpoint_sup.sh \u251c\u2500\u2500 src \u2502 \u251c\u2500\u2500 analysis \u2502 \u2514\u2500\u2500 simulation \u2514\u2500\u2500 tests src/analysis : Contains the code for visualizing and analyzing results from the simulation experiments. data : This folder contains executables, experimental results, and analysis results. A user will rarely interact with this folder directly. Most scripts assume this directory and all its contents exist. src/simulation : Contains the code for running experiments with simulated agents. Following is the structure of simulation folder: ``` \u251c\u2500\u2500 agent \u251c\u2500\u2500 algorithms \u251c\u2500\u2500 callback \u251c\u2500\u2500 common \u251c\u2500\u2500 conf \u251c\u2500\u2500 env_wrapper \u251c\u2500\u2500 networks \u251c\u2500\u2500 pycache \u251c\u2500\u2500 run_parsing_exp.py \u251c\u2500\u2500 run_parsing_icm.py \u251c\u2500\u2500 run_viewpoint_exp.py \u2514\u2500\u2500 utils.py `` * Unity : Contains a Unity project which is a virtual replication of the VR Chambers used in contolled-rearing studies from the lab. This folder should be opened as a Unity project in the Unity Editor. * tests : Contains unit tests to test the environment and others * docs : Contains project documentation * scripts`: Contains bash scripts to run the respective experiment","title":"Directory Structure"},{"location":"#how-to-install","text":"In this section, you will pull this repository from Github, open the Unity environment, and build the ChickAI environment as an executable.","title":"How to Install"},{"location":"#codebase-installation","text":"Install Git and/or Github Desktop. If you install Git, you'll be able to interact with Github through the command line. You can download Git using the directions here: https://git-scm.com/downloads . If you install Git Desktop, you can use a GUI to interact with Github. You can install Github Desktop by following the directions here: https://docs.github.com/en/desktop/installing-and-configuring-github-desktop/ . For the following steps, I will provide the command line arguments (but you can use the GUI to find the same options in Github Desktop). To download the repository, click the Code button on the pipeline_embodied repo. Copy the provided web URL. Then follow the code below to change to directory where you want the repo (denoted here as MY_FOLDER) and then clone the repo. cd MY_FOLDER git clone URL_YOU_COPIED_GOES_HERE 3. Checkout the branch you want to be extra sure that you're using the right branch. cd pipeline_embodied git checkout DESIRED_BRANCH 4. (Highly Recommended) create and configure a virtual environment *steps described below : * ``` conda create -n pipeline_embodied_env python=3.8 conda activate pipeline_embodied_env git clone git@github.com:buildingamind/pipeline_embodied.git cd pipeline_embodied pip install -r requirements.txt ```","title":"Codebase Installation"},{"location":"#running-an-experiment-default-configuration","text":"After having followed steps 1-5 above once experiments can be run with a few lines of code bash scripts/<EXPERIMENT_NAME>_sup.sh where EXPERIMENT_NAME is one of the experiments ( viewpoint , binding , parsing ) using supervised reward.**","title":"Running an Experiment (default configuration)"},{"location":"#running-standard-analysis","text":"After running the experiments, the pipeline will generate a collection of datafiles in the Data folder. To run the analyses performed in the papers you can use the following command python3 src/analysis/run.py This will generate a collection of graphs in the folder data/Results .","title":"Running Standard Analysis"},{"location":"#running-an-experiment-custom-configuration","text":"After having replicated the results with the default configuration, you may wish to experiment by plugging in different brains for the agent. We have included a few different possible brains for the agent. To plug these in simply modify the yaml file in src/simulation/conf/Agent/basic.yaml . encoder: BRAIN where BRAIN can be set to small , medium , or large which correspond to a 4-layer CNN, 10-Layer ResNet, and 18-layer ResNet respectively. Note that if you change the size of the encoder, you may also consider changing the number of training episodes. This can be done in the config file src/simulation/conf/config.yaml . train_eps: NEW_EPISODE_COUNT If you wish to experiment with custom architectures or a new policy network, this can be done by modifying the agent script ( Simulation/agent.py ). self.model is the policy network and self.encoder is the encoder network. Both can be assigned any appropriately sized torch.nn.module .","title":"Running an Experiment (custom configuration)"},{"location":"#experiment-configuration","text":"More information related to details on the experiment can be found on following pages. Parsing Experiment ViewPoint Experiment","title":"Experiment Configuration"},{"location":"Parsing/","text":"A newborn embodied Turing test for visual parsing Manju Garimella, Denizhan Pak, Lalit Pandey, Justin N. Wood, & Samantha M. W. Wood Parsing Abstract Newborn brains exhibit remarkable abilities in rapid and generative learning, including the ability to parse objects from backgrounds and recognize those objects across substantial changes to their appearance (i.e., novel backgrounds and novel viewing angles). How can we build machines that can learn as efficiently as newborns? To accurately compare biological and artificial intelligence, researchers need to provide machines with the same training data that an organism has experienced since birth. Here, we present an experimental benchmark that enables researchers to raise artificial agents in the same controlled-rearing environments as newborn chicks. First, we raised newborn chicks in controlled environments with visual access to only a single object on a single background and tested their ability to recognize their object across novel viewing conditions. Then, we performed \u201cdigital twin\u201d experiments in which we reared a variety of artificial neural networks in virtual environments that mimicked the rearing conditions of the chicks and measured whether they exhibited the same object recognition behavior as the newborn chicks. We found that biological chicks developed background-invariant object recognition, while the artificial chicks developed background-dependent recognition. Our benchmark exposes the limitations of current unsupervised and supervised algorithms in achieving the learning abilities of newborn animals. Ultimately, we anticipate that this approach will contribute to the development of AI systems that can learn with the same efficiency as newborn animals. Experiment Design VR chambers were equipped with two display walls (LCD monitors) for displaying object stimuli. During the Training Phase, artificial chicks were reared in an environment containing a single 3D object rotating a full 360\u00b0 around a horizontal axis in front of a naturalistic background scene. The object made a full rotation every 15s. During the Test Phase, the VR chambers measured the artificial chicks\u2019 imprinting response and object recognition performance. The \u201cimprinting trials\u201d measured whether the chicks developed an imprinting response. The \u201ctest trials\u201d measured the aritifical chicks\u2019 ability to visually parse and recognize their imprinted object. During these trials, the imprinted object was presented on one display wall and an unfamiliar object was presented on the other display wall. Across the test trials, the objects were presented on all possible combinations of the three background scenes (Background 1 vs.Background 1, Background 1 vs. Background 2, Background 1 vs.Background 3, etc.). Arguments Train configuration agent_count: 1 run_id:exp1 log_path: data/exp1 mode: full train_eps: 1000 test_eps: 40 cuda: 0 Agent: reward: supervised encoder: small Environment: use_ship: true side_view: false background: A base_port: 5100 env_path: data/executables/parsing_benchmark/parsing.x86_64 log_path: data/ship_backgroundA_exp/Env_Logs rec_path: data/ship_backgroundA_exp/Recordings/ record_chamber: false record_agent: false recording_frames: 0 Run script python src/simulation/run_parsing_exp.py ++run_id=exp1 ++Environment.env_path=data/executables/parsing_benchmark/parsing_app.x86_64 ++mode=full ++train_eps=1000 ++test_eps=40 ++Agent.encoder=\"small\" ++Environment.use_ship=\"true\" ++Environment.background=\"A\" where Environment.use_ship = True or False (to choose between Ship and Fork); Environment.background = A, B, C (to choose between the three background); mode = full or train or test (to choose between the three modes to run); Agent.encoder = \"small\", \"medium\" or \"large\" to choose between the three different types of encoders: NatureCNN, resnet10 and resnet18 Agent.reward=\"supervised\" default Custom Configuration: Update train episode count; test episode count Encoder types - small, medium and large Reward types - supervised or 'unsupervised' Links Exectuables can be found here","title":"Parsing"},{"location":"Parsing/#a-newborn-embodied-turing-test-for-visual-parsing","text":"Manju Garimella, Denizhan Pak, Lalit Pandey, Justin N. Wood, & Samantha M. W. Wood Parsing","title":"A newborn embodied Turing test for visual parsing"},{"location":"Parsing/#abstract","text":"Newborn brains exhibit remarkable abilities in rapid and generative learning, including the ability to parse objects from backgrounds and recognize those objects across substantial changes to their appearance (i.e., novel backgrounds and novel viewing angles). How can we build machines that can learn as efficiently as newborns? To accurately compare biological and artificial intelligence, researchers need to provide machines with the same training data that an organism has experienced since birth. Here, we present an experimental benchmark that enables researchers to raise artificial agents in the same controlled-rearing environments as newborn chicks. First, we raised newborn chicks in controlled environments with visual access to only a single object on a single background and tested their ability to recognize their object across novel viewing conditions. Then, we performed \u201cdigital twin\u201d experiments in which we reared a variety of artificial neural networks in virtual environments that mimicked the rearing conditions of the chicks and measured whether they exhibited the same object recognition behavior as the newborn chicks. We found that biological chicks developed background-invariant object recognition, while the artificial chicks developed background-dependent recognition. Our benchmark exposes the limitations of current unsupervised and supervised algorithms in achieving the learning abilities of newborn animals. Ultimately, we anticipate that this approach will contribute to the development of AI systems that can learn with the same efficiency as newborn animals.","title":"Abstract"},{"location":"Parsing/#experiment-design","text":"VR chambers were equipped with two display walls (LCD monitors) for displaying object stimuli. During the Training Phase, artificial chicks were reared in an environment containing a single 3D object rotating a full 360\u00b0 around a horizontal axis in front of a naturalistic background scene. The object made a full rotation every 15s. During the Test Phase, the VR chambers measured the artificial chicks\u2019 imprinting response and object recognition performance. The \u201cimprinting trials\u201d measured whether the chicks developed an imprinting response. The \u201ctest trials\u201d measured the aritifical chicks\u2019 ability to visually parse and recognize their imprinted object. During these trials, the imprinted object was presented on one display wall and an unfamiliar object was presented on the other display wall. Across the test trials, the objects were presented on all possible combinations of the three background scenes (Background 1 vs.Background 1, Background 1 vs. Background 2, Background 1 vs.Background 3, etc.).","title":"Experiment Design"},{"location":"Parsing/#arguments","text":"","title":"Arguments"},{"location":"Parsing/#train-configuration","text":"agent_count: 1 run_id:exp1 log_path: data/exp1 mode: full train_eps: 1000 test_eps: 40 cuda: 0 Agent: reward: supervised encoder: small Environment: use_ship: true side_view: false background: A base_port: 5100 env_path: data/executables/parsing_benchmark/parsing.x86_64 log_path: data/ship_backgroundA_exp/Env_Logs rec_path: data/ship_backgroundA_exp/Recordings/ record_chamber: false record_agent: false recording_frames: 0","title":"Train configuration"},{"location":"Parsing/#run-script","text":"python src/simulation/run_parsing_exp.py ++run_id=exp1 ++Environment.env_path=data/executables/parsing_benchmark/parsing_app.x86_64 ++mode=full ++train_eps=1000 ++test_eps=40 ++Agent.encoder=\"small\" ++Environment.use_ship=\"true\" ++Environment.background=\"A\" where Environment.use_ship = True or False (to choose between Ship and Fork); Environment.background = A, B, C (to choose between the three background); mode = full or train or test (to choose between the three modes to run); Agent.encoder = \"small\", \"medium\" or \"large\" to choose between the three different types of encoders: NatureCNN, resnet10 and resnet18 Agent.reward=\"supervised\" default","title":"Run script"},{"location":"Parsing/#custom-configuration","text":"Update train episode count; test episode count Encoder types - small, medium and large Reward types - supervised or 'unsupervised'","title":"Custom Configuration:"},{"location":"Parsing/#links","text":"Exectuables can be found here","title":"Links"},{"location":"ViewInvariant/","text":"A newborn embodied Turing test for View-Invariant Recognition Denizhan Pak, Donsuk Lee, Samantha M. W. Wood & Justin N. Wood https://github.com/buildingamind/pipeline_embodied/assets/1686251/2fed4649-b4d6-4c93-813c-cd040a92c8cb Abstract Recent progress in artificial intelligence has renewed interest in building machines that learn like animals. Almost all of the work comparing learning across biological and artificial systems comes from studies where animals and machines received different training data, obscuring whether differences between animals and machines emerged from differences in learning mechanisms versus training data. We present an experimental approach\u2014a \u201cnewborn embodied Turing Test\u201d\u2014that allows newborn animals and machines to be raised in the same environments and tested with the same tasks, permitting direct comparison of their learning abilities. To make this platform, we first collected controlled-rearing data from newborn chicks, then performed \u201cdigital twin\u201d experiments in which machines were raised in virtual environments that mimicked the rearing conditions of the chicks. We found that (1) machines (deep reinforcement learning agents with intrinsic motivation) can spontaneously develop visually guided preference behavior, akin to imprinting in newborn chicks, and (2) machines are still far from newborn-level performance on object recognition tasks. Almost all of the chicks developed view-invariant object recognition, whereas the machines tended to develop view-dependent recognition. The learning outcomes were also far more constrained in the chicks versus machines. Ultimately, we anticipate that this approach will help researchers develop embodied AI systems that learn like newborn animals. Experiment Design VR chambers were equipped with two display walls (LCD monitors) for displaying object stimuli. During the Training Phase, artificial chicks were reared in an environment containing a single 3D object rotating 15\u00b0 around a vertical axis in front of a blank background scene. The object made a full rotation every 3s. Agents can be imprinted to one of 4 possible conditions: side and front views of the Fork object or side and front views of the ship object. During the Test Phase, the VR chambers measured the artificial chicks\u2019 imprinting response and object recognition performance. The \u201cimprinting trials\u201d measured whether the chicks developed an imprinting response. The \u201ctest trials\u201d measured the aritifical chicks\u2019 ability to visually discriminate their imprinted object. During these trials, the imprinted object, rotated at an alternate angle to the imprint condition, was presented on one display wall and an unfamiliar object was presented on the other display wall, the angle of which was either the same as the imprint condition (fixed trials) or matched to the viewpoint in the test condition (matched trials). Arguments Train configuration agent_count: 1 run_id:ship_front_exp log_path: data/ship_front_exp mode: full train_eps: 1000 test_eps: 40 cuda: 0 Agent: reward: supervised encoder: small Environment: use_ship: true side_view: false background: A base_port: 5100 env_path: data/executables/viewpoint_benchmark/viewpoint.x86_64 log_path: data/ship_front_exp/Env_Logs rec_path: data/ship_front_exp/Recordings/ record_chamber: false record_agent: false recording_frames: 0 Executables Exectuable can be found here .","title":"ViewInvariant"},{"location":"ViewInvariant/#a-newborn-embodied-turing-test-for-view-invariant-recognition","text":"Denizhan Pak, Donsuk Lee, Samantha M. W. Wood & Justin N. Wood https://github.com/buildingamind/pipeline_embodied/assets/1686251/2fed4649-b4d6-4c93-813c-cd040a92c8cb","title":"A newborn embodied Turing test for View-Invariant Recognition"},{"location":"ViewInvariant/#abstract","text":"Recent progress in artificial intelligence has renewed interest in building machines that learn like animals. Almost all of the work comparing learning across biological and artificial systems comes from studies where animals and machines received different training data, obscuring whether differences between animals and machines emerged from differences in learning mechanisms versus training data. We present an experimental approach\u2014a \u201cnewborn embodied Turing Test\u201d\u2014that allows newborn animals and machines to be raised in the same environments and tested with the same tasks, permitting direct comparison of their learning abilities. To make this platform, we first collected controlled-rearing data from newborn chicks, then performed \u201cdigital twin\u201d experiments in which machines were raised in virtual environments that mimicked the rearing conditions of the chicks. We found that (1) machines (deep reinforcement learning agents with intrinsic motivation) can spontaneously develop visually guided preference behavior, akin to imprinting in newborn chicks, and (2) machines are still far from newborn-level performance on object recognition tasks. Almost all of the chicks developed view-invariant object recognition, whereas the machines tended to develop view-dependent recognition. The learning outcomes were also far more constrained in the chicks versus machines. Ultimately, we anticipate that this approach will help researchers develop embodied AI systems that learn like newborn animals.","title":"Abstract"},{"location":"ViewInvariant/#experiment-design","text":"VR chambers were equipped with two display walls (LCD monitors) for displaying object stimuli. During the Training Phase, artificial chicks were reared in an environment containing a single 3D object rotating 15\u00b0 around a vertical axis in front of a blank background scene. The object made a full rotation every 3s. Agents can be imprinted to one of 4 possible conditions: side and front views of the Fork object or side and front views of the ship object. During the Test Phase, the VR chambers measured the artificial chicks\u2019 imprinting response and object recognition performance. The \u201cimprinting trials\u201d measured whether the chicks developed an imprinting response. The \u201ctest trials\u201d measured the aritifical chicks\u2019 ability to visually discriminate their imprinted object. During these trials, the imprinted object, rotated at an alternate angle to the imprint condition, was presented on one display wall and an unfamiliar object was presented on the other display wall, the angle of which was either the same as the imprint condition (fixed trials) or matched to the viewpoint in the test condition (matched trials).","title":"Experiment Design"},{"location":"ViewInvariant/#arguments","text":"","title":"Arguments"},{"location":"ViewInvariant/#train-configuration","text":"agent_count: 1 run_id:ship_front_exp log_path: data/ship_front_exp mode: full train_eps: 1000 test_eps: 40 cuda: 0 Agent: reward: supervised encoder: small Environment: use_ship: true side_view: false background: A base_port: 5100 env_path: data/executables/viewpoint_benchmark/viewpoint.x86_64 log_path: data/ship_front_exp/Env_Logs rec_path: data/ship_front_exp/Recordings/ record_chamber: false record_agent: false recording_frames: 0","title":"Train configuration"},{"location":"ViewInvariant/#executables","text":"Exectuable can be found here .","title":"Executables"}]}