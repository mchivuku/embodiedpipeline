<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="canonical" href="https://origin.luddy.indiana.edu/Parsing/" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Parsing - Embodied Pipeline Benchmarks</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Parsing";
        var mkdocs_page_input_path = "Parsing.md";
        var mkdocs_page_url = "/Parsing/";
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> Embodied Pipeline Benchmarks
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="" href="../experiments.md">Experiments</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="./">Parsing</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#abstract">Abstract</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#experiment-design">Experiment Design</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#arguments">Arguments</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#train-configuration">Train configuration</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#run-script">Run script</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#custom-configuration">Custom Configuration:</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#links">Links</a>
    </li>
    </ul>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">Embodied Pipeline Benchmarks</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Parsing</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h2 id="a-newborn-embodied-turing-test-for-visual-parsing">A newborn embodied Turing test for visual parsing</h2>
<p>Manju Garimella, Denizhan Pak, Lalit Pandey, Justin N. Wood, &amp; Samantha M. W. Wood</p>
<p><a href="https://github.com/buildingamind/pipeline_embodied/assets/1686251/839bd04c-8853-44c4-b275-5e61413a3904">Parsing</a></p>
<h3 id="abstract">Abstract</h3>
<p><em>Newborn brains exhibit remarkable abilities in rapid and generative learning, including the ability to parse objects from backgrounds and recognize those objects across substantial changes to their appearance (i.e., novel backgrounds and novel viewing angles). How can we build machines that can learn as efficiently as newborns? To accurately compare biological and artificial intelligence, researchers need to provide machines with the same training data that an organism has experienced since birth. Here, we present an experimental benchmark that enables researchers to raise artificial agents in the same controlled-rearing environments as newborn chicks. First, we raised newborn chicks in controlled environments with visual access to only a single object on a single background and tested their ability to recognize their object across novel viewing conditions. Then, we performed “digital twin” experiments in which we reared a variety of artificial neural networks in virtual environments that mimicked the rearing conditions of the chicks and measured whether they exhibited the same object recognition behavior as the newborn chicks. We found that biological chicks developed background-invariant object recognition, while the artificial chicks developed background-dependent recognition. Our benchmark exposes the limitations of current unsupervised and supervised algorithms in achieving the learning abilities of newborn animals. Ultimately, we anticipate that this approach will contribute to the development of AI systems that can learn with the same efficiency as newborn animals.</em></p>
<h3 id="experiment-design">Experiment Design</h3>
<ul>
<li>VR chambers were equipped with two display walls (LCD monitors) for displaying object stimuli.</li>
<li>During the Training Phase, artificial chicks were reared in an environment containing a single 3D object rotating a full 360° around a horizontal axis in front of a naturalistic background scene. The object made a full rotation every 15s.</li>
<li>During the Test Phase, the VR chambers measured the artificial chicks’ imprinting response and object recognition performance. The “imprinting trials” measured whether the chicks developed an imprinting response.  The “test trials” measured the aritifical chicks’ ability to visually parse and recognize their imprinted object. During these trials, the imprinted object was presented on one display wall and an unfamiliar object was presented on the other display wall. Across the test trials, the objects were presented on all possible combinations of the three background scenes (Background 1 vs.Background 1, Background 1 vs. Background 2, Background 1 vs.Background 3, etc.).</li>
</ul>
<h3 id="arguments">Arguments</h3>
<h4 id="train-configuration">Train configuration</h4>
<pre><code>agent_count: 1
run_id:exp1
log_path: data/exp1
mode: full
train_eps: 1000
test_eps: 40
cuda: 0
Agent:
  reward: supervised
  encoder: small
Environment:
  use_ship: true
  side_view: false
  background: A
  base_port: 5100
  env_path: data/executables/parsing_benchmark/parsing.x86_64
  log_path: data/ship_backgroundA_exp/Env_Logs
  rec_path: data/ship_backgroundA_exp/Recordings/
  record_chamber: false
  record_agent: false
  recording_frames: 0
</code></pre>
<h5 id="run-script">Run script</h5>
<pre><code class="language-bash">python src/simulation/run_parsing_exp.py ++run_id=exp1 ++Environment.env_path=data/executables/parsing_benchmark/parsing_app.x86_64 ++mode=full ++train_eps=1000 ++test_eps=40 ++Agent.encoder=&quot;small&quot; ++Environment.use_ship=&quot;true&quot; ++Environment.background=&quot;A&quot;

where
Environment.use_ship = True or False (to choose between Ship and Fork);
Environment.background = A, B, C (to choose between the three background);
mode = full or train or test (to choose between the three modes to run);
Agent.encoder = &quot;small&quot;, &quot;medium&quot; or &quot;large&quot; to choose between the three different types of encoders: NatureCNN, resnet10 and resnet18
Agent.reward=&quot;supervised&quot; default

</code></pre>
<h5 id="custom-configuration">Custom Configuration:</h5>
<ul>
<li>Update train episode count; test episode count</li>
<li>Encoder types - small, medium and large</li>
<li>Reward types - supervised or 'unsupervised'</li>
</ul>
<h3 id="links">Links</h3>
<p><a href="https://origins.luddy.indiana.edu/unity/executables/">Exectuables can be found here</a></p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href=".." class="btn btn-neutral float-left" title="Home"><span class="icon icon-circle-arrow-left"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href=".." style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
